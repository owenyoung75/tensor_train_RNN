{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor RNN for long term forecasting demo \n",
    "- seq2seq architecture with tensor RNN cell\n",
    "- variable length of forecasting horizon\n",
    "- flexible temporal high order structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1: Flags for training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flags configuration loaded ...\n",
      "loading time series ...\n",
      "input type  <class 'numpy.ndarray'> (1000, 100, 1)\n",
      "normalize to (0-1)\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "|input steps| 12 |out steps| 88 |hidden size| 2 |learning rate| 0.001 |orders| 8 |virtual-D| 1 |time lag| 2 |layers| 1\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Functions for downloading and reading time series data.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "from tensorflow.python.framework import random_seed\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from reader import read_data_sets\n",
    "from model_seq2seq import *\n",
    "from trnn2 import *\n",
    "import numpy\n",
    "from train_config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "flags = tf.flags\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel2')\n",
    "flags.DEFINE_string(\"model\", \"SymmetricTLSTM\", \"Model used for learning.\")\n",
    "flags.DEFINE_string(\"data_path\", \"./data.npy\", \"Data input directory.\")\n",
    "flags.DEFINE_string(\"save_path\", \"./log/tlstm/\", \"Model output directory.\")\n",
    "flags.DEFINE_integer(\"inp_steps\", 12, \"burn in steps\")\n",
    "flags.DEFINE_integer(\"out_steps\", 88, \"test steps\")\n",
    "flags.DEFINE_integer(\"hidden_size\", 2, \"hidden layer size\")\n",
    "flags.DEFINE_float(\"learning_rate\", 1e-3, \"learning rate\")\n",
    "flags.DEFINE_float(\"decay_rate\", 0.8, \"decay rate\")\n",
    "flags.DEFINE_integer(\"virtual_dim\", 1, \"dimension of virtual legs\")\n",
    "flags.DEFINE_integer(\"num_orders\", 8, \"order of polynomials\")\n",
    "flags.DEFINE_integer(\"num_lags\", 2, \"time-lag length\")\n",
    "flags.DEFINE_integer(\"num_layers\", 1, \"time-lag length\")\n",
    "flags.DEFINE_integer(\"batch_size\", 50, \"batch size\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "print('Flags configuration loaded ...')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training Parameters\n",
    "config = TrainConfig()\n",
    "config.hidden_size = FLAGS.hidden_size\n",
    "config.learning_rate = FLAGS.learning_rate\n",
    "config.decay_rate = FLAGS.decay_rate\n",
    "config.virtual_dim = FLAGS.virtual_dim\n",
    "config.num_orders = FLAGS.num_orders\n",
    "config.num_lags = FLAGS.num_lags\n",
    "config.num_layers = FLAGS.num_layers\n",
    "config.inp_steps = FLAGS.inp_steps\n",
    "config.out_steps = FLAGS.out_steps\n",
    "config.batch_size = FLAGS.batch_size\n",
    "\n",
    "\n",
    "# Training Parameters\n",
    "training_steps = config.training_steps\n",
    "batch_size = config.batch_size\n",
    "display_step = 500\n",
    "inp_steps = config.inp_steps\n",
    "out_steps = config.out_steps\n",
    "\n",
    "\n",
    "# Read Dataset\n",
    "dataset, stats = read_data_sets(FLAGS.data_path, True, inp_steps, out_steps)\n",
    "num_input = stats['num_input']  # dataset data input (time series dimension: 1)\n",
    "num_steps = stats['num_steps']\n",
    "\n",
    "\n",
    "# Print training config\n",
    "print('-'*120)\n",
    "print('|input steps|', inp_steps,\n",
    "      '|out steps|', out_steps,\n",
    "      '|hidden size|', config.hidden_size,\n",
    "      '|learning rate|', config.learning_rate,\n",
    "      '|orders|', config.num_orders,\n",
    "      '|virtual-D|', config.virtual_dim,\n",
    "      '|time lag|', config.num_lags,\n",
    "      '|layers|', config.num_layers\n",
    "      )\n",
    "print('-'*120)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build neural network models\n",
    "Building seq2seq model for training/validation/testing,\n",
    "validation and testing models are shared,\n",
    "scheduled sampling is by default off "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training -->\n",
      "          Create Encoder ...\n",
      "          Create Decoder ...\n",
      "Testing -->\n",
      "          Create Encoder ...\n",
      "          Create Decoder ...\n",
      "Testing -->\n",
      "          Create Encoder ...\n",
      "          Create Decoder ...\n",
      "<function SymmetricTLSTM at 0x181f05b840>Model built ...\n"
     ]
    }
   ],
   "source": [
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, inp_steps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, out_steps, num_input])\n",
    "Z = tf.placeholder(\"float\", [None, out_steps, num_input])\n",
    "\n",
    "\n",
    "# batch_x, batch_y, batch_z = dataset.train.next_batch(batch_size)\n",
    "# X = tf.convert_to_tensor(batch_x, dtype=tf.float32)\n",
    "# Y = tf.convert_to_tensor(batch_y, dtype=tf.float32)\n",
    "# X = X[:, 0:5, :]\n",
    "# Y = Y[:, 0:5, :]\n",
    "# Z = Y\n",
    "\n",
    "Model = globals()[FLAGS.model]\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Train\"):\n",
    "    with tf.variable_scope(\"TLSTM\", reuse=False):\n",
    "        train_pred = Model(X, Y, True,  config)\n",
    "with tf.name_scope(\"Valid\"):\n",
    "    with tf.variable_scope(\"TLSTM\", reuse=True):\n",
    "        valid_pred = Model(X, Y, False,  config)\n",
    "with tf.name_scope(\"Test\"):\n",
    "    with tf.variable_scope(\"TLSTM\", reuse=True):\n",
    "        test_pred = Model(X, Y, False,  config)\n",
    "       \n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "train_loss = tf.sqrt(tf.reduce_mean(tf.squared_difference(train_pred, Z)))\n",
    "valid_loss = tf.sqrt(tf.reduce_mean(tf.squared_difference(valid_pred, Z)))\n",
    "test_loss = tf.sqrt(tf.reduce_mean(tf.squared_difference(test_pred, Z)))\n",
    "\n",
    "# Exponential learning rate decay \n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = config.learning_rate\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, \n",
    "                                           global_step,\n",
    "                                           2000, \n",
    "                                           config.decay_rate, \n",
    "                                           staircase=True)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(train_loss, global_step=global_step)\n",
    "\n",
    "# # Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Saver for the model and loss\n",
    "saver = tf.train.Saver()\n",
    "hist_loss =[]\n",
    "\n",
    "print( str(Model) + 'Model built ...' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 0.3626\n",
      "Validation Loss: 0.36195406\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f75bc473b783>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                  feed_dict={X: batch_x, \n\u001b[1;32m     11\u001b[0m                             \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                             Z: batch_z}\n\u001b[0m\u001b[1;32m     13\u001b[0m                 )\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ipykernel_py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ipykernel_py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ipykernel_py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ipykernel_py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ipykernel_py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ipykernel_py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_steps = 1000\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)    \n",
    "    \n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y, batch_z = dataset.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, \n",
    "                 feed_dict={X: batch_x, \n",
    "                            Y: batch_y, \n",
    "                            Z: batch_z}\n",
    "                )\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss \n",
    "            tr_loss = sess.run(train_loss,\n",
    "                               feed_dict={X: batch_x, \n",
    "                                          Y: batch_y, \n",
    "                                          Z:batch_z}\n",
    "                              )\n",
    "            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \"{:.4f}\".format(tr_loss) )\n",
    "            \n",
    "            # Calculate validation\n",
    "            valid_enc_inps = dataset.validation.enc_inps.reshape((-1, inp_steps, num_input))\n",
    "            valid_dec_inps = dataset.validation.dec_inps.reshape((-1, out_steps, num_input))\n",
    "            valid_dec_outs = dataset.validation.dec_outs.reshape((-1, out_steps, num_input))\n",
    "            va_loss = sess.run(valid_loss,\n",
    "                               feed_dict={X: valid_enc_inps,\n",
    "                                          Y: valid_dec_inps, \n",
    "                                          Z: valid_dec_outs}\n",
    "                              )\n",
    "            print(\"Validation Loss:\", va_loss)\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for test datasets\n",
    "    test_enc_inps = dataset.test.enc_inps.reshape((-1, inp_steps, num_input))\n",
    "    test_dec_inps = dataset.test.dec_inps.reshape((-1, out_steps, num_input))\n",
    "    test_dec_outs = dataset.test.dec_outs.reshape((-1, out_steps, num_input))\n",
    "\n",
    "    true, pred, loss = sess.run([Z, test_pred, test_loss],\n",
    "                                feed_dict={X: test_enc_inps, \n",
    "                                           Y: test_dec_inps, \n",
    "                                           Z: test_dec_outs}\n",
    "                               )\n",
    "    print(\"Testing Loss:\", loss)\n",
    "\n",
    "    # Save the variables to disk.\n",
    "    save_path = saver.save(sess, FLAGS.save_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    # Save predictions \n",
    "    numpy.save(save_path+\"predict.npy\", (true, pred))\n",
    "    # Save config file\n",
    "    with open(save_path+\"config.out\", 'w') as f:\n",
    "        f.write('hidden_size:'+ str(config.hidden_size)+'\\n'+ 'learning_rate:'+ str(config.learning_rate)+ '\\n')\n",
    "        f.write('train_error:'+ str(loss) +'\\n'+ 'valid_error:' + str(va_loss) + '\\n'+ 'test_error:'+ str(loss) +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "y_true = true\n",
    "y_pred = pred\n",
    "plt.plot(y_true[0,:,0].T,':')\n",
    "plt.plot(y_pred[0,:,0].T,'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
